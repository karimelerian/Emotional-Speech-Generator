{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyRFomUNJu2L"
      },
      "source": [
        "# Training the model on specific emotional datasets (Cloning)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TTS\n",
        "!pip install TTS\n",
        "!pip install coqui-tts\n",
        "!pip uninstall torchvision -y"
      ],
      "metadata": {
        "id": "is9WBLw1LuWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxYJNgeKhMHu",
        "outputId": "8afe8927-41b6-4ac3-9e98-ebed2e724263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'ravdess-emotional-speech-audio' dataset.\n",
            "Total files downloaded: 2880\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# 1. Download the full dataset\n",
        "data_path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "\n",
        "# The actual audio files are usually nested inside a specific folder within the download path\n",
        "# Use glob to find all .wav files in the downloaded structure\n",
        "all_files = glob(os.path.join(data_path, '**', '*.wav'), recursive=True)\n",
        "\n",
        "# 2. Define target emotions and actor\n",
        "# We will choose a single actor (e.g., Actor 12 - female)\n",
        "# And a set of target emotions we want to test\n",
        "TARGET_ACTOR = '12'\n",
        "\n",
        "# We will use the 'strong' intensity (02) for maximum style transfer\n",
        "# Emotion IDs: 01=Neutral, 03=Happy, 04=Sad, 05=Angry, 06=Fearful\n",
        "TARGET_EMOTIONS = ['01', '03', '04', '05', '06']\n",
        "\n",
        "print(f\"Total files downloaded: {len(all_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmAFuxJbhR1A",
        "outputId": "47d480ea-53ff-44b1-a1e0-28013ee477fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Selected Reference Clips ---\n",
            "[Sad]: 03-01-04-02-02-02-12.wav copied to ravdess_reference_clips/sad_ref_actor12.wav\n",
            "[Neutral]: 03-01-01-01-02-02-12.wav copied to ravdess_reference_clips/neutral_ref_actor12.wav\n",
            "[Happy]: 03-01-03-02-02-01-12.wav copied to ravdess_reference_clips/happy_ref_actor12.wav\n",
            "[Angry]: 03-01-05-02-01-01-12.wav copied to ravdess_reference_clips/angry_ref_actor12.wav\n",
            "[Fearful]: 03-01-06-02-02-01-12.wav copied to ravdess_reference_clips/fearful_ref_actor12.wav\n",
            "\n",
            "Preparation complete. 5 clips ready in the 'ravdess_reference_clips' directory.\n"
          ]
        }
      ],
      "source": [
        "# 3. Create a dictionary to store the best reference clip found for each emotion\n",
        "reference_clips = {}\n",
        "EMOTION_MAP = {\n",
        "    '01': 'Neutral', '02': 'Calm', '03': 'Happy', '04': 'Sad',\n",
        "    '05': 'Angry', '06': 'Fearful', '07': 'Disgust', '08': 'Surprised'\n",
        "}\n",
        "\n",
        "# 4. Loop through all files and apply the filters\n",
        "for file_path in all_files:\n",
        "    filename = os.path.basename(file_path)\n",
        "\n",
        "    # Split the filename identifier (e.g., ['03', '01', '06', '01', '02', '01', '12'])\n",
        "    parts = filename.split('.')[0].split('-')\n",
        "\n",
        "    # Sanity check: ensure it's a speech file\n",
        "    if parts[1] != '01': # Vocal channel (01 = speech)\n",
        "        continue\n",
        "\n",
        "    emotion_id = parts[2]\n",
        "    intensity_id = parts[3]\n",
        "    actor_id = parts[6]\n",
        "\n",
        "    # Filter 1: Check if the file is from the target actor\n",
        "    if actor_id != TARGET_ACTOR:\n",
        "        continue\n",
        "\n",
        "    # Filter 2: Check if the file is a target emotion\n",
        "    if emotion_id not in TARGET_EMOTIONS:\n",
        "        continue\n",
        "\n",
        "    # Filter 3: Prioritize Strong intensity (02), except for Neutral (which only has 01)\n",
        "    # We will take the first one found that meets the criteria\n",
        "    if (emotion_id == '01' and intensity_id == '01') or \\\n",
        "       (emotion_id != '01' and intensity_id == '02'):\n",
        "\n",
        "        emotion_name = EMOTION_MAP[emotion_id]\n",
        "\n",
        "        # We only need one example per emotion\n",
        "        if emotion_name not in reference_clips:\n",
        "            reference_clips[emotion_name] = file_path\n",
        "\n",
        "# 5. Print the selected files and create a clean directory for them\n",
        "output_dir = 'ravdess_reference_clips'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(\"\\n--- Selected Reference Clips ---\")\n",
        "\n",
        "for emotion, source_path in reference_clips.items():\n",
        "\n",
        "    # Create a clean, easy-to-use filename for the Coqui model\n",
        "    target_filename = f\"{emotion.lower()}_ref_actor{TARGET_ACTOR}.wav\"\n",
        "    target_path = os.path.join(output_dir, target_filename)\n",
        "\n",
        "    # Copy the file to the new directory\n",
        "    os.system(f\"cp \\\"{source_path}\\\" \\\"{target_path}\\\"\")\n",
        "\n",
        "    print(f\"[{emotion}]: {os.path.basename(source_path)} copied to {target_path}\")\n",
        "\n",
        "print(f\"\\nPreparation complete. {len(reference_clips)} clips ready in the '{output_dir}' directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri4vPKTPkNqR",
        "outputId": "759680f0-3f2b-4f3b-d717-8a0c7e14ef1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Loading XTTS-v2 model...\n",
            " > You must confirm the following:\n",
            " | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n",
            " | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n",
            " | | > y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.87G/1.87G [00:39<00:00, 47.8MiB/s]\n",
            "4.37kiB [00:00, 3.70MiB/s]\n",
            "361kiB [00:00, 27.8MiB/s]\n",
            "100%|██████████| 32.0/32.0 [00:00<00:00, 53.2kiB/s]\n",
            "100%|██████████| 7.75M/7.75M [00:00<00:00, 49.8MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from TTS.api import TTS\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# 1. Define Model and Target Text\n",
        "MODEL_NAME = \"tts_models/multilingual/multi-dataset/xtts_v2\"\n",
        "TARGET_TEXT = \"The fox jumps over the lazy dog.\"\n",
        "LANGUAGE = \"en\" # English\n",
        "\n",
        "# 2. Get device (use CUDA if available for speed)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 3. Initialize XTTS model\n",
        "# This will download the model weights if not already present (can take a few minutes the first time)\n",
        "print(\"Loading XTTS-v2 model...\")\n",
        "tts = TTS(MODEL_NAME).to(device)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# 4. Define your reference and output directories\n",
        "REF_DIR = 'ravdess_reference_clips'\n",
        "OUTPUT_DIR_XTTS = 'generated_xtts_cloning'\n",
        "os.makedirs(OUTPUT_DIR_XTTS, exist_ok=True)\n",
        "\n",
        "# List of your prepared reference files:\n",
        "# This list corresponds to the files you generated in the previous step's output.\n",
        "reference_files = [f for f in os.listdir(REF_DIR) if f.endswith('.wav')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3QKP3dHnvSN",
        "outputId": "4ee8fabe-287c-4958-9838-225d4dbd0abe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting XTTS Generation (Voice Cloning) ---\n",
            "Generating [Neutral] using neutral_ref_actor12.wav...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS: Neutral_XTTS_Clone.wav\n",
            "Generating [Angry] using angry_ref_actor12.wav...\n",
            "SUCCESS: Angry_XTTS_Clone.wav\n",
            "Generating [Happy] using happy_ref_actor12.wav...\n",
            "SUCCESS: Happy_XTTS_Clone.wav\n",
            "Generating [Fearful] using fearful_ref_actor12.wav...\n",
            "SUCCESS: Fearful_XTTS_Clone.wav\n",
            "Generating [Sad] using sad_ref_actor12.wav...\n",
            "SUCCESS: Sad_XTTS_Clone.wav\n",
            "\n",
            "XTTS generation complete. 5 samples saved in 'generated_xtts_cloning'.\n"
          ]
        }
      ],
      "source": [
        "# 5. Define target text, directories, and list of references\n",
        "TARGET_TEXT = \"Mert is amazing, I love Karim.\"\n",
        "REF_DIR = 'ravdess_reference_clips'\n",
        "OUTPUT_DIR_XTTS = 'generated_xtts_cloning'\n",
        "# Note: You already created the output directory and defined reference_files earlier,\n",
        "# but we'll redefine the files list here in case the kernel was reset.\n",
        "\n",
        "import os\n",
        "# Assumes files are in the ravdess_reference_clips directory\n",
        "reference_files = [f for f in os.listdir(REF_DIR) if f.endswith('.wav')]\n",
        "os.makedirs(OUTPUT_DIR_XTTS, exist_ok=True)\n",
        "\n",
        "# 6. Loop through each reference clip and generate the emotional output\n",
        "print(\"\\n--- Starting XTTS Generation (Voice Cloning) ---\")\n",
        "for ref_file in reference_files:\n",
        "    # Example ref_file: sad_ref_actor12.wav\n",
        "    emotion_name = ref_file.split('_')[0].capitalize()\n",
        "    ref_path = os.path.join(REF_DIR, ref_file)\n",
        "\n",
        "    output_filename = f\"{emotion_name}_XTTS_Clone.wav\"\n",
        "    output_path = os.path.join(OUTPUT_DIR_XTTS, output_filename)\n",
        "\n",
        "    print(f\"Generating [{emotion_name}] using {ref_file}...\")\n",
        "\n",
        "    try:\n",
        "        # XTTS-v2 Generation command: uses the ref_path as the speaker_wav\n",
        "        tts.tts_to_file(\n",
        "            text=TARGET_TEXT,\n",
        "            speaker_wav=ref_path,\n",
        "            language=\"en\", # Must match the language used for the model and text\n",
        "            file_path=output_path,\n",
        "        )\n",
        "        print(f\"SUCCESS: {output_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR generating {emotion_name}: {e}\")\n",
        "\n",
        "print(f\"\\nXTTS generation complete. {len(reference_files)} samples saved in '{OUTPUT_DIR_XTTS}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "\n",
        "# --- NEW CONFIGURATION ---\n",
        "# The folder containing your synthesized emotional audio files (from the XTTS step)\n",
        "output_folder_xtts = \"generated_xtts_cloning\"\n",
        "generated_files_xtts = [os.path.join(output_folder_xtts, f) for f in os.listdir(output_folder_xtts) if f.endswith(\".wav\")]\n",
        "\n",
        "# Define your expected emotions for comparison (using the XTTS naming convention)\n",
        "EXPECTED_EMOTIONS_XTTS = {\n",
        "    'Angry_XTTS_Clone.wav': 'Angry',\n",
        "    'Fearful_XTTS_Clone.wav': 'Fearful',\n",
        "    'Happy_XTTS_Clone.wav': 'Happy',\n",
        "    'Neutral_XTTS_Clone.wav': 'Neutral',\n",
        "    'Sad_XTTS_Clone.wav': 'Sad',\n",
        "}\n",
        "# -------------------------\n",
        "\n",
        "# --- 1. Load the Speech Emotion Recognition (SER) Pipeline (REUSED) ---\n",
        "# NOTE: If the kernel has been restarted, you must re-run the SER model loading part first!\n",
        "print(\"Loading Speech Emotion Recognition model...\")\n",
        "try:\n",
        "    emotion_classifier = pipeline(\n",
        "        \"audio-classification\",\n",
        "        model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\",\n",
        "        device=-1 # Run on CPU\n",
        "    )\n",
        "    TARGET_SR = emotion_classifier.feature_extractor.sampling_rate\n",
        "    print(f\"SER model loaded successfully. Target Sample Rate: {TARGET_SR} Hz\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load SER model. Error: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Predict Emotion for Each XTTS Generated File ---\n",
        "\n",
        "print(\"\\n=== XTTS Emotion Recognition Results ===\")\n",
        "print(\"{:<25} {:<10} {:<10} {:<15}\".format(\"Audio File\", \"Target\", \"Predicted\", \"Confidence\"))\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for file in generated_files_xtts:\n",
        "    filename = os.path.basename(file)\n",
        "    target_emotion = EXPECTED_EMOTIONS_XTTS.get(filename, \"N/A\")\n",
        "\n",
        "    try:\n",
        "        # Load audio using torchaudio (the safe, pure-Python way)\n",
        "        audio_data, sr = torchaudio.load(file)\n",
        "\n",
        "        # Resample if needed to match the SER model's requirement\n",
        "        if sr != TARGET_SR:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=TARGET_SR)\n",
        "            audio_data = resampler(audio_data)\n",
        "\n",
        "        # Convert to a 1D NumPy array for the classifier\n",
        "        raw_audio_array = audio_data.mean(dim=0).cpu().numpy()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {filename} with torchaudio: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Pass the raw NumPy array (not the file path) to the classifier\n",
        "    prediction = emotion_classifier(raw_audio_array)\n",
        "\n",
        "    # Process the top prediction\n",
        "    if prediction:\n",
        "        predicted_emotion = prediction[0]['label'].capitalize()\n",
        "        confidence = prediction[0]['score']\n",
        "\n",
        "        match_status = \"✅ Match\" if predicted_emotion == target_emotion else \"❌ Mismatch\"\n",
        "\n",
        "        print(\"{:<25} {:<10} {:<10} {:<15.3f} ({})\".format(\n",
        "            filename,\n",
        "            target_emotion,\n",
        "            predicted_emotion,\n",
        "            confidence,\n",
        "            match_status\n",
        "        ))\n",
        "\n",
        "print(\"\\nXTTS Evaluation complete. Compare these results to the Chatterbox results!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625,
          "referenced_widgets": [
            "3c17873cbfa943ec88eee62fc75fa497",
            "4482afcf506c4ddd827c2e84fdaf36c5",
            "a87a748be9ff4ffb8e8b7d1246358fae",
            "700186c9762b454b8fbe04c528d45673",
            "41ad3895bfea4765849918a355a5f7c1",
            "4c37fe63f257476ebb666259bbe29bef",
            "13ef67afe8bc4a7193bf4173e88240ac",
            "8bf149a3093b49b9a0a912e936be8473",
            "6058fb727fb443ed8af07cbff783f6f4",
            "707ee7d26ee140d1a35767b9d84b96cd",
            "b6749a3f56924a92af4a649cbe3e49b4",
            "d9b93aacd1f849bd8f60a55a9150d8cf",
            "8ec2b2c083b740f5aeea02cf471b3d28",
            "769177c162594198b0e6adabe4251630",
            "bc45b0972a6c43cc95829a5cfba03537",
            "ec6279919b36442eb9705fd05f829def",
            "4f9febe1ec924e8d96398c0cee2d3943",
            "dbdfe34ad65e4a6fa574973aaf2475d5",
            "1d6912a8a46c4e6ab640b2c5c8785951",
            "1737379f2e144d4987319a5d13d4959e",
            "f95e61e1303a48e881305c141de3a8d0",
            "35d7ee1984864e10a336856e1ab639b2",
            "ec20f8fc97e846e784327e0cc683a340",
            "9b950f2f43534d89aba69dc483242daa",
            "fe432aa231624139a1124effff81ab9c",
            "30f29808f66043518ee8dfad8fac81c1",
            "22a85174e3444f6ea5951aa1a86ddea2",
            "1c7df10a674045da9d89019132275b73",
            "ecce76bc95854594a4dc7641e54aaefd",
            "589fb4b417a241b3ba99b42a39a7411e",
            "7a12f930a9304db1b0bddf9a4006a7ec",
            "40e7d8a7f1084623985619ce011b250c",
            "5e20a7204cb34fa2882479789c9e304a"
          ]
        },
        "id": "0oUqXVzwNRkD",
        "outputId": "f20dc8d2-54ce-4f1e-eee9-975117798f54"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Speech Emotion Recognition model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c17873cbfa943ec88eee62fc75fa497"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:334: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9b93aacd1f849bd8f60a55a9150d8cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec20f8fc97e846e784327e0cc683a340"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SER model loaded successfully. Target Sample Rate: 16000 Hz\n",
            "\n",
            "=== XTTS Emotion Recognition Results ===\n",
            "Audio File                Target     Predicted  Confidence     \n",
            "------------------------------------------------------------\n",
            "Angry_XTTS_Clone.wav      Angry      Sad        0.140           (❌ Mismatch)\n",
            "Fearful_XTTS_Clone.wav    Fearful    Fearful    0.131           (✅ Match)\n",
            "Neutral_XTTS_Clone.wav    Neutral    Neutral    0.133           (✅ Match)\n",
            "Happy_XTTS_Clone.wav      Happy      Sad        0.131           (❌ Mismatch)\n",
            "Sad_XTTS_Clone.wav        Sad        Sad        0.139           (✅ Match)\n",
            "\n",
            "XTTS Evaluation complete. Compare these results to the Chatterbox results!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx0TBkFzOQPE",
        "outputId": "07f0625b-41b4-421c-e302-dc724e8b33e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.13.1.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=81f0c9ffc06cf5d3cf22c8127c51a63852d6f56104f5e0f4ccb91e188fe65df1\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: rapidfuzz, jiwer, openai-whisper\n",
            "Successfully installed jiwer-4.0.0 openai-whisper-20250625 rapidfuzz-3.14.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import whisper\n",
        "import os\n",
        "from jiwer import wer\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "# Load Whisper\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Your target text\n",
        "TARGET_TEXT = \"Mert is amazing, I love Karim.\" # Corrected case/punctuation for consistency\n",
        "\n",
        "# Folder with your generated audios\n",
        "# VVVV --- CHANGE THIS LINE --- VVVV\n",
        "output_folder = \"generated_xtts_cloning\"\n",
        "# ^^^^ ------------------------ ^^^^\n",
        "generated_files = [os.path.join(output_folder, f) for f in os.listdir(output_folder) if f.endswith(\".wav\")]\n",
        "\n",
        "print(\"=== XTTS WER Evaluation (Using torchaudio to bypass FFmpeg) ===\")\n",
        "for file in generated_files:\n",
        "    try:\n",
        "        # Load audio data and its sample rate (sr)\n",
        "        audio_data, sr = torchaudio.load(file)\n",
        "\n",
        "        # Resample to 16000 Hz, which is Whisper's required sample rate\n",
        "        if sr != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
        "            audio_data = resampler(audio_data)\n",
        "\n",
        "        # Take the mean across channels (if stereo) and convert to 1D tensor/array\n",
        "        audio_tensor = audio_data.mean(dim=0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {os.path.basename(file)} with torchaudio: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Transcribe using the loaded audio tensor (bypasses whisper's internal audio loader)\n",
        "    # Whisper expects a numpy array\n",
        "    result = model.transcribe(audio_tensor.cpu().numpy())\n",
        "\n",
        "    transcription = result[\"text\"]\n",
        "    # Calculate Word Error Rate\n",
        "    score = wer(TARGET_TEXT.lower(), transcription.lower())\n",
        "\n",
        "    print(f\"{os.path.basename(file)} -> WER: {score:.3f}, Transcription: {transcription}\")\n",
        "\n",
        "print(\"\\nEvaluation complete. XTTS WER results are ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHZ-AnD3Ntux",
        "outputId": "6a7e435f-bf8c-48b8-cb23-60efefcdc2db"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:05<00:00, 26.5MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== XTTS WER Evaluation (Using torchaudio to bypass FFmpeg) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Angry_XTTS_Clone.wav -> WER: 0.333, Transcription:  MIRT is amazing. I love Karim.\n",
            "Fearful_XTTS_Clone.wav -> WER: 0.500, Transcription:  Merism-bazing, I love Karim.\n",
            "Neutral_XTTS_Clone.wav -> WER: 0.333, Transcription:  Merch is amazing, I love Kareem.\n",
            "Happy_XTTS_Clone.wav -> WER: 0.500, Transcription:  MIRT is amazing. I love KERIP.\n",
            "Sad_XTTS_Clone.wav -> WER: 0.500, Transcription:  Merit is amazing. I love current.\n",
            "\n",
            "Evaluation complete. XTTS WER results are ready!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "state": {},
      "application/vnd.jupyter.widget-state+json": {
        "3c17873cbfa943ec88eee62fc75fa497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4482afcf506c4ddd827c2e84fdaf36c5",
              "IPY_MODEL_a87a748be9ff4ffb8e8b7d1246358fae",
              "IPY_MODEL_700186c9762b454b8fbe04c528d45673"
            ],
            "layout": "IPY_MODEL_41ad3895bfea4765849918a355a5f7c1"
          }
        },
        "4482afcf506c4ddd827c2e84fdaf36c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c37fe63f257476ebb666259bbe29bef",
            "placeholder": "​",
            "style": "IPY_MODEL_13ef67afe8bc4a7193bf4173e88240ac",
            "value": "config.json: "
          }
        },
        "a87a748be9ff4ffb8e8b7d1246358fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bf149a3093b49b9a0a912e936be8473",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6058fb727fb443ed8af07cbff783f6f4",
            "value": 1
          }
        },
        "700186c9762b454b8fbe04c528d45673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_707ee7d26ee140d1a35767b9d84b96cd",
            "placeholder": "​",
            "style": "IPY_MODEL_b6749a3f56924a92af4a649cbe3e49b4",
            "value": " 2.28k/? [00:00&lt;00:00, 23.6kB/s]"
          }
        },
        "41ad3895bfea4765849918a355a5f7c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c37fe63f257476ebb666259bbe29bef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13ef67afe8bc4a7193bf4173e88240ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bf149a3093b49b9a0a912e936be8473": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6058fb727fb443ed8af07cbff783f6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "707ee7d26ee140d1a35767b9d84b96cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6749a3f56924a92af4a649cbe3e49b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9b93aacd1f849bd8f60a55a9150d8cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ec2b2c083b740f5aeea02cf471b3d28",
              "IPY_MODEL_769177c162594198b0e6adabe4251630",
              "IPY_MODEL_bc45b0972a6c43cc95829a5cfba03537"
            ],
            "layout": "IPY_MODEL_ec6279919b36442eb9705fd05f829def"
          }
        },
        "8ec2b2c083b740f5aeea02cf471b3d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f9febe1ec924e8d96398c0cee2d3943",
            "placeholder": "​",
            "style": "IPY_MODEL_dbdfe34ad65e4a6fa574973aaf2475d5",
            "value": "model.safetensors: 100%"
          }
        },
        "769177c162594198b0e6adabe4251630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d6912a8a46c4e6ab640b2c5c8785951",
            "max": 1266038848,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1737379f2e144d4987319a5d13d4959e",
            "value": 1266038848
          }
        },
        "bc45b0972a6c43cc95829a5cfba03537": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f95e61e1303a48e881305c141de3a8d0",
            "placeholder": "​",
            "style": "IPY_MODEL_35d7ee1984864e10a336856e1ab639b2",
            "value": " 1.27G/1.27G [00:23&lt;00:00, 124MB/s]"
          }
        },
        "ec6279919b36442eb9705fd05f829def": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f9febe1ec924e8d96398c0cee2d3943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbdfe34ad65e4a6fa574973aaf2475d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d6912a8a46c4e6ab640b2c5c8785951": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1737379f2e144d4987319a5d13d4959e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f95e61e1303a48e881305c141de3a8d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d7ee1984864e10a336856e1ab639b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec20f8fc97e846e784327e0cc683a340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b950f2f43534d89aba69dc483242daa",
              "IPY_MODEL_fe432aa231624139a1124effff81ab9c",
              "IPY_MODEL_30f29808f66043518ee8dfad8fac81c1"
            ],
            "layout": "IPY_MODEL_22a85174e3444f6ea5951aa1a86ddea2"
          }
        },
        "9b950f2f43534d89aba69dc483242daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c7df10a674045da9d89019132275b73",
            "placeholder": "​",
            "style": "IPY_MODEL_ecce76bc95854594a4dc7641e54aaefd",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "fe432aa231624139a1124effff81ab9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_589fb4b417a241b3ba99b42a39a7411e",
            "max": 214,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a12f930a9304db1b0bddf9a4006a7ec",
            "value": 214
          }
        },
        "30f29808f66043518ee8dfad8fac81c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40e7d8a7f1084623985619ce011b250c",
            "placeholder": "​",
            "style": "IPY_MODEL_5e20a7204cb34fa2882479789c9e304a",
            "value": " 214/214 [00:00&lt;00:00, 8.88kB/s]"
          }
        },
        "22a85174e3444f6ea5951aa1a86ddea2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c7df10a674045da9d89019132275b73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecce76bc95854594a4dc7641e54aaefd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "589fb4b417a241b3ba99b42a39a7411e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a12f930a9304db1b0bddf9a4006a7ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40e7d8a7f1084623985619ce011b250c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e20a7204cb34fa2882479789c9e304a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}